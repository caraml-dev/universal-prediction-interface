syntax = "proto3";

package caraml.upi.v1;

/******************************************
************        Log        ************
*******************************************/
// LogType supported by UPI
enum LogType {
    LOG_TYPE_UNSPECIFIED = 0;
    // Log from Prediction service
    LOG_TYPE_PREDICTION = 1;
    // Log from Observation service
    LOG_TYPE_OBSERVATION = 2;
    // Log from Router service
    LOG_TYPE_ROUTER = 3;
}

// LogProducer describes the service that generates the log
message LogProducer {
    // Unique identifier of the producer.
    string id = 1;
    // Name of the producer, dependent on the type of the log.
    string name = 2;
    // Name of the CaraML project that hosts the producer.
    string project = 3;
}

// Log is an entity/metadata in Dataset Service that represents an append-only
// data produced by ingesting the observation, prediction, or router logs
message Log {
    // Unique identifier of a log generated by a LogProducer.
    string id = 1;
    // Name of the log, generated by Dataset Service.
    string name = 2;
    // Source of the log.
    LogType type = 3;
    // List of target names associated with a log.
    repeated string target_names = 4;
    // BQ table ID where the data is stored.
    string bq_table = 5;
    // Details of LogProducer that generated a log.
    LogProducer log_producer = 6;
}

/******************************************
***********        Shared       ***********
*******************************************/
// Data sink where logs would be flushed to via Fluentd
enum FluentdOutputType {
    FLUENTD_OUTPUT_TYPE_UNSPECIFIED = 0;
    // Fluentd will publish logs to standard output
    FLUENTD_OUTPUT_TYPE_STDOUT = 1;
    // Fluentd will flush logs to BigQuery
    FLUENTD_OUTPUT_TYPE_BQ = 2;
}

// Fluentd BQ Data sink configurations
message FluentdOutputBQConfig {
    // GCP Project
    string project = 1;
    // GCP Dataset
    string dataset = 2;
    // GCP Table
    string table = 3;
}

// Fluentd Data sink configurations
message FluentdConfig {
    // The type of Data Sink where Observation logs would be flushed to
    FluentdOutputType type = 1;
    // Fluentd Host to connect to
    string host = 2;
    // Fluentd Port to connect to
    int32 port = 3;
    // Fluentd Tag to match messages
    string tag = 4;

    FluentdOutputBQConfig config = 5;
}

// Kafka configurations
message KafkaConfig {
    // Kafka Brokers to connect to, comma-delimited, in the form of "<broker_host>:<broker_port>"
    string brokers = 1;
    // Kafka Topic to produce to/consume from
    string topic = 2;
    // Largest record batch size allowed by Kafka (after compression if compression is enabled)
    int64 max_message_bytes = 3;
    // The compression type for all data generated by the Producer
    string compression_type = 4;
    // ConnectTimeoutMS is the maximum duration (ms) the Kafka Producer/Consumer will block for to get Metadata, before timing out
    int32 connection_timeout = 5;
    // PollInterval is the maximum duration (ms) the Kafka Consumer will block for, before timing out
    int32 poll_interval = 6;
    // What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server
    string offset_reset = 7;
}
